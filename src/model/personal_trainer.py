#!/usr/bin/env python3
"""
–ü–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–∞—à–µ–≥–æ —Å—Ç–∏–ª—è –æ–±—â–µ–Ω–∏—è
"""

import torch
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling
)
from peft import LoraConfig, get_peft_model, TaskType
from datasets import Dataset
import pandas as pd
from pathlib import Path
import json
from typing import Dict, List
import numpy as np


class PersonalizedStyleTrainer:
    """–¢—Ä–µ–Ω–µ—Ä –¥–ª—è –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è"""

    def __init__(self, config):
        self.config = config
        self.device = config.models.device if torch.cuda.is_available() else "cpu"
        self.base_model_name = config.models.base_model

        # –î–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        self.adapters_dir = Path(config.paths.adapters_dir)
        self.style_profiles_dir = Path(config.paths.style_profiles)
        self.adapters_dir.mkdir(parents=True, exist_ok=True)

        # –ú–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä—ã
        self.tokenizer = None
        self.base_model = None

    def load_my_style_data(self) -> Dict:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –æ –º–æ–µ–º —Å—Ç–∏–ª–µ –æ–±—â–µ–Ω–∏—è"""
        print("üìÅ –ó–∞–≥—Ä—É–∑–∫–∞ –≤–∞—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö –æ —Å—Ç–∏–ª–µ –æ–±—â–µ–Ω–∏—è...")

        style_profile = self.style_profiles_dir / "my_style_profile.json"
        cloned_data = Path(self.config.paths.cloned_data) / "my_communication_style.json"

        my_data = {}

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–æ—Ñ–∏–ª—å —Å—Ç–∏–ª—è
        if style_profile.exists():
            with open(style_profile, 'r', encoding='utf-8') as f:
                my_data['style_profile'] = json.load(f)
            print("‚úÖ –ü—Ä–æ—Ñ–∏–ª—å —Å—Ç–∏–ª—è –∑–∞–≥—Ä—É–∂–µ–Ω")

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–ª–æ–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è
        if cloned_data.exists():
            with open(cloned_data, 'r', encoding='utf-8') as f:
                my_data['messages'] = json.load(f)
            print(f"‚úÖ –°–æ–æ–±—â–µ–Ω–∏—è –∑–∞–≥—Ä—É–∂–µ–Ω—ã: {len(my_data['messages'].get('messages', []))}")

        return my_data

    def prepare_personalized_dataset(self, my_data: Dict) -> pd.DataFrame:
        """–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç"""
        print("üìä –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞...")

        if 'messages' not in my_data or 'messages' not in my_data['messages']:
            print("‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
            return pd.DataFrame()

        messages = my_data['messages']['messages']

        data = []

        for msg in messages:
            text = msg.get('text', '')
            context_type = msg.get('context_type', 'unknown')
            context_messages = msg.get('context_messages', [])

            if not text or len(text) < 3:
                continue

            # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
            if context_messages:
                context = " | ".join(context_messages[-3:])  # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 3 —Å–æ–æ–±—â–µ–Ω–∏—è
            else:
                context = f"–ö–æ–Ω—Ç–µ–∫—Å—Ç: {context_type}"

            # –°–æ–∑–¥–∞–µ–º –ø—Ä–∏–º–µ—Ä –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
            example = {
                'prompt': f"""–°—Ç–∏–ª—å –æ–±—â–µ–Ω–∏—è: {context_type}
–ö–æ–Ω—Ç–µ–∫—Å—Ç –¥–∏–∞–ª–æ–≥–∞: {context}

–¢–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –≤ —Å–≤–æ–µ–º —Å—Ç–∏–ª–µ:""",
                'response': text,
                'context_type': context_type,
                'context': context,
                'message_length': len(text.split()),
                'has_emoji': any(emoji in text for emoji in ['üòÄ', 'üòÇ', 'üòä', 'üòç', 'üòé'])
            }

            data.append(example)

        df = pd.DataFrame(data)

        print(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ –ø—Ä–∏–º–µ—Ä–æ–≤: {len(df)}")
        print(f"üìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞–º:")
        print(df['context_type'].value_counts())

        return df

    def train_context_adapter(self, context_type: str, context_df: pd.DataFrame):
        """–û–±—É—á–∞–µ—Ç –∞–¥–∞–ø—Ç–µ—Ä –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
        if context_df.empty or len(context_df) < 10:
            print(f"‚ö†Ô∏è  –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ {context_type}")
            return None

        print(f"\nüéØ –û–±—É—á–µ–Ω–∏–µ –∞–¥–∞–ø—Ç–µ—Ä–∞ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞: {context_type}")
        print(f"üìä –ü—Ä–∏–º–µ—Ä–æ–≤: {len(context_df)}")

        # –°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç
        from datasets import Dataset

        def format_example(row):
            return {
                'text': f"{row['prompt']}\n{row['response']}{self.tokenizer.eos_token}"
            }

        formatted_data = []
        for _, row in context_df.iterrows():
            formatted_data.append(format_example(row))

        dataset_df = pd.DataFrame(formatted_data)
        dataset = Dataset.from_pandas(dataset_df)

        # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º
        def tokenize_function(examples):
            return self.tokenizer(
                examples['text'],
                truncation=True,
                padding="max_length",
                max_length=256
            )

        tokenized_data = dataset.map(tokenize_function, batched=True)

        # –°–æ–∑–¥–∞–µ–º LoRA –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        lora_config = LoraConfig(
            r=8,
            lora_alpha=32,
            target_modules=["c_attn", "c_proj"],
            lora_dropout=0.1,
            bias="none",
            task_type=TaskType.CAUSAL_LM
        )

        # –°–æ–∑–¥–∞–µ–º –∞–¥–∞–ø—Ç–µ—Ä
        model = get_peft_model(self.base_model, lora_config)
        model.print_trainable_parameters()

        # –ê—Ä–≥—É–º–µ–Ω—Ç—ã –æ–±—É—á–µ–Ω–∏—è
        training_args = TrainingArguments(
            output_dir=str(self.adapters_dir / f"{context_type}_checkpoints"),
            num_train_epochs=3,
            per_device_train_batch_size=2,
            per_device_eval_batch_size=2,
            warmup_steps=50,
            weight_decay=0.01,
            logging_dir=str(Path(self.config.paths.logs_dir) / context_type),
            logging_steps=10,
            save_strategy="epoch",
            eval_strategy="no",
            report_to="none",
            fp16=self.device == "cuda",
        )

        # –¢—Ä–µ–Ω–µ—Ä
        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=tokenized_data,
            data_collator=DataCollatorForLanguageModeling(
                tokenizer=self.tokenizer,
                mlm=False
            ),
        )

        # –û–±—É—á–∞–µ–º
        print(f"üöÄ –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è...")
        trainer.train()

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∞–¥–∞–ø—Ç–µ—Ä
        adapter_path = self.adapters_dir / f"{context_type}_adapter"
        model.save_pretrained(adapter_path)

        print(f"‚úÖ –ê–¥–∞–ø—Ç–µ—Ä —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {adapter_path}")

        return str(adapter_path)

    def train_general_style_adapter(self, df: pd.DataFrame):
        """–û–±—É—á–∞–µ—Ç –æ–±—â–∏–π –∞–¥–∞–ø—Ç–µ—Ä –º–æ–µ–≥–æ —Å—Ç–∏–ª—è"""
        print("\nüé≠ –û–±—É—á–µ–Ω–∏–µ –æ–±—â–µ–≥–æ –∞–¥–∞–ø—Ç–µ—Ä–∞ –º–æ–µ–≥–æ —Å—Ç–∏–ª—è...")

        if df.empty or len(df) < 20:
            print("‚ùå –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –æ–±—â–µ–≥–æ —Å—Ç–∏–ª—è")
            return None

        # –°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç
        from datasets import Dataset

        def format_example(row):
            return {
                'text': f"{row['prompt']}\n{row['response']}{self.tokenizer.eos_token}"
            }

        formatted_data = []
        for _, row in df.iterrows():
            formatted_data.append(format_example(row))

        dataset_df = pd.DataFrame(formatted_data)
        dataset = Dataset.from_pandas(dataset_df)

        # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º
        def tokenize_function(examples):
            return self.tokenizer(
                examples['text'],
                truncation=True,
                padding="max_length",
                max_length=256
            )

        tokenized_data = dataset.map(tokenize_function, batched=True)

        # –°–æ–∑–¥–∞–µ–º LoRA –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        lora_config = LoraConfig(
            r=16,  # –ë–æ–ª—å—à–∏–π rank –¥–ª—è –æ–±—â–µ–≥–æ —Å—Ç–∏–ª—è
            lora_alpha=64,
            target_modules=["c_attn", "c_proj", "c_fc"],
            lora_dropout=0.1,
            bias="none",
            task_type=TaskType.CAUSAL_LM
        )

        # –°–æ–∑–¥–∞–µ–º –∞–¥–∞–ø—Ç–µ—Ä
        model = get_peft_model(self.base_model, lora_config)
        model.print_trainable_parameters()

        # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ train –∏ validation
        split_dataset = tokenized_data.train_test_split(test_size=0.1, seed=42)

        # –ê—Ä–≥—É–º–µ–Ω—Ç—ã –æ–±—É—á–µ–Ω–∏—è
        training_args = TrainingArguments(
            output_dir=str(self.adapters_dir / "my_style_checkpoints"),
            num_train_epochs=5,  # –ë–æ–ª—å—à–µ —ç–ø–æ—Ö –¥–ª—è –æ–±—â–µ–≥–æ —Å—Ç–∏–ª—è
            per_device_train_batch_size=2,
            per_device_eval_batch_size=2,
            warmup_steps=100,
            weight_decay=0.01,
            logging_dir=str(Path(self.config.paths.logs_dir) / "my_style"),
            logging_steps=10,
            save_strategy="epoch",
            eval_strategy="epoch",
            save_total_limit=2,
            load_best_model_at_end=True,
            metric_for_best_model="eval_loss",
            greater_is_better=False,
            report_to="none",
            fp16=self.device == "cuda",
        )

        # –¢—Ä–µ–Ω–µ—Ä
        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=split_dataset["train"],
            eval_dataset=split_dataset["test"],
            data_collator=DataCollatorForLanguageModeling(
                tokenizer=self.tokenizer,
                mlm=False
            ),
        )

        # –û–±—É—á–∞–µ–º
        print(f"üöÄ –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è –æ–±—â–µ–≥–æ —Å—Ç–∏–ª—è...")
        trainer.train()

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∞–¥–∞–ø—Ç–µ—Ä
        adapter_path = self.adapters_dir / "my_style_adapter"
        model.save_pretrained(adapter_path)

        print(f"‚úÖ –û–±—â–∏–π –∞–¥–∞–ø—Ç–µ—Ä —Å—Ç–∏–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {adapter_path}")

        return str(adapter_path)

    def train(self):
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç –ø–æ–ª–Ω–æ–µ –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ"""
        print("üéØ –ù–ê–ß–ê–õ–û –ü–ï–†–°–û–ù–ê–õ–ò–ó–ò–†–û–í–ê–ù–ù–û–ì–û –û–ë–£–ß–ï–ù–ò–Ø")
        print("=" * 50)

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å
        self._load_base_model()

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–∏ –¥–∞–Ω–Ω—ã–µ
        my_data = self.load_my_style_data()

        if not my_data:
            print("‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
            return

        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç
        df = self.prepare_personalized_dataset(my_data)

        if df.empty:
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –¥–∞—Ç–∞—Å–µ—Ç")
            return

        # –û–±—É—á–∞–µ–º –∞–¥–∞–ø—Ç–µ—Ä—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        context_adapters = {}

        for context_type in df['context_type'].unique():
            if context_type == 'unknown':
                continue

            context_df = df[df['context_type'] == context_type]

            if len(context_df) >= self.config.training.personalization.min_messages_per_style:
                adapter_path = self.train_context_adapter(context_type, context_df)
                if adapter_path:
                    context_adapters[context_type] = adapter_path

        # –û–±—É—á–∞–µ–º –æ–±—â–∏–π –∞–¥–∞–ø—Ç–µ—Ä –º–æ–µ–≥–æ —Å—Ç–∏–ª—è
        general_adapter = self.train_general_style_adapter(df)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –∞–¥–∞–ø—Ç–µ—Ä–∞—Ö
        adapters_info = {
            'base_model': self.base_model_name,
            'general_style_adapter': general_adapter,
            'context_adapters': context_adapters,
            'training_date': pd.Timestamp.now().isoformat(),
            'training_statistics': {
                'total_examples': len(df),
                'context_distribution': df['context_type'].value_counts().to_dict(),
                'average_message_length': df['message_length'].mean(),
                'emoji_frequency': df['has_emoji'].mean()
            }
        }

        info_file = self.adapters_dir / "personalized_adapters_info.json"
        with open(info_file, 'w', encoding='utf-8') as f:
            json.dump(adapters_info, f, ensure_ascii=False, indent=2)

        print(f"\n‚úÖ –ü–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
        print(f"üìä –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: {info_file}")
        print(f"üé≠ –û–±—É—á–µ–Ω–æ –∞–¥–∞–ø—Ç–µ—Ä–æ–≤: {len(context_adapters)} –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤ + –æ–±—â–∏–π —Å—Ç–∏–ª—å")

        return adapters_info

    def _load_base_model(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä"""
        print(f"üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏: {self.base_model_name}")

        self.tokenizer = AutoTokenizer.from_pretrained(self.base_model_name)

        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token

        self.base_model = AutoModelForCausalLM.from_pretrained(
            self.base_model_name,
            torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
            device_map="auto" if self.device == "cuda" else None,
            tie_word_embeddings=False
        )

        print(f"‚úÖ –ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")


def main():
    """–¢–æ—á–∫–∞ –≤—Ö–æ–¥–∞ –¥–ª—è –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è"""
    import yaml
    from pathlib import Path

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
    config_path = Path("config.yaml")
    if not config_path.exists():
        print("‚ùå –§–∞–π–ª –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω")
        return

    with open(config_path, 'r', encoding='utf-8') as f:
        config_dict = yaml.safe_load(f)

    # –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ
    print("üöÄ –ó–∞–ø—É—Å–∫ –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è...")

    try:
        trainer = PersonalizedStyleTrainer(config_dict)
        trainer.train()
    except KeyboardInterrupt:
        print("\nüëã –û–±—É—á–µ–Ω–∏–µ –ø—Ä–µ—Ä–≤–∞–Ω–æ")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è: {e}")


if __name__ == "__main__":
    main()